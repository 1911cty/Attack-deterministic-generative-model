{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please be aware that the official implementation of StyTr2 may not be able to used directly, as it may encounter errors related to gradient backpropagation failures. Please comment out the part after Ics in the stytr.py.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from os.path import basename\n",
    "from os.path import splitext\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from function import calc_mean_std, normal, coral\n",
    "import models.transformer as transformer\n",
    "import models.StyTR as StyTR\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from function import normal\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "new_state_dict = OrderedDict()\n",
    "\n",
    "import clip\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=\"cuda\")\n",
    "\n",
    "import cv2 as cv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_transform(size, crop):\n",
    "    transform_list = []\n",
    "   \n",
    "    if size != 0: \n",
    "        transform_list.append(transforms.Resize(size))\n",
    "    if crop:\n",
    "        transform_list.append(transforms.CenterCrop(size))\n",
    "    transform_list.append(transforms.ToTensor())\n",
    "    transform = transforms.Compose(transform_list)\n",
    "    return transform\n",
    "def style_transform(h,w):\n",
    "    k = (h,w)\n",
    "    size = int(np.max(k))\n",
    "    print(type(size))\n",
    "    transform_list = []    \n",
    "    transform_list.append(transforms.CenterCrop((h,w)))\n",
    "    transform_list.append(transforms.ToTensor())\n",
    "    transform = transforms.Compose(transform_list)\n",
    "    return transform\n",
    "\n",
    "def content_transform():\n",
    "    \n",
    "    transform_list = []   \n",
    "    transform_list.append(transforms.ToTensor())\n",
    "    transform = transforms.Compose(transform_list)\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_size=512\n",
    "style_size=512\n",
    "crop='store_true'\n",
    "save_ext='.jpg'\n",
    "preserve_color='store_true'\n",
    "alpha=10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = StyTR.vgg\n",
    "vgg.load_state_dict(torch.load('./experiments/vgg_normalised.pth'))\n",
    "vgg = nn.Sequential(*list(vgg.children())[:44])\n",
    "\n",
    "decoder = StyTR.decoder\n",
    "Trans = transformer.Transformer()\n",
    "embedding = StyTR.PatchEmbed()\n",
    "decoder.eval()\n",
    "Trans.eval()\n",
    "vgg.eval()\n",
    "print('init')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load('experiments/decoder_iter_160000.pth')\n",
    "\n",
    "for k, v in state_dict.items():\n",
    "    #namekey = k[7:] # remove `module.`\n",
    "    namekey = k\n",
    "    new_state_dict[namekey] = v\n",
    "decoder.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state_dict = OrderedDict()\n",
    "state_dict = torch.load('experiments/transformer_iter_160000.pth')\n",
    "for k, v in state_dict.items():\n",
    "    #namekey = k[7:] # remove `module.`\n",
    "    namekey = k\n",
    "    new_state_dict[namekey] = v\n",
    "Trans.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state_dict = OrderedDict()\n",
    "state_dict = torch.load('experiments/embedding_iter_160000.pth')\n",
    "for k, v in state_dict.items():\n",
    "    #namekey = k[7:] # remove `module.`\n",
    "    namekey = k\n",
    "    new_state_dict[namekey] = v\n",
    "embedding.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "network = StyTR.StyTrans(vgg,decoder,embedding,Trans,None)\n",
    "\n",
    "network.eval()\n",
    "\n",
    "network.to(device)\n",
    "\n",
    "print('init done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_tf = test_transform(content_size, crop)\n",
    "style_tf = test_transform(style_size, crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "content_path = 'content/content.jpg'\n",
    "style_path = 'style/style.jpg'\n",
    "output_path='/output/'\n",
    "\n",
    "img_size = \n",
    "\n",
    "content = content_tf(Image.open(content_path).convert(\"RGB\"))\n",
    "h,w,c=np.shape(content)    \n",
    "style = style_tf(Image.open(style_path).convert(\"RGB\"))\n",
    "\n",
    "\n",
    "style = style.to(device).unsqueeze(0)\n",
    "style += torch.rand_like(style) / 1e7\n",
    "content = content.to(device).unsqueeze(0)\n",
    "content += torch.rand_like(content) / 1e7\n",
    "\n",
    "\n",
    "style = nn.functional.interpolate(style, (img_size,img_size))\n",
    "content = nn.functional.interpolate(content, (img_size,img_size))\n",
    "\n",
    "\n",
    "style.requires_grad_(True)\n",
    "style.retain_grad()\n",
    "content.requires_grad_(True)\n",
    "content.retain_grad()\n",
    "\n",
    "\n",
    "word = 'puzzle'\n",
    "word_original = 'oil painting'\n",
    "\n",
    "\n",
    "word_token = clip.tokenize([word, word_original]).cuda()\n",
    "text_feature = clip_model.encode_text(word_token)\n",
    "text_feature_delta = text_feature[0:1] - text_feature[1:2]\n",
    "\n",
    "\n",
    "eps = \n",
    "\n",
    "batch={}\n",
    "batch['style'] = style\n",
    "batch['content'] = content\n",
    "\n",
    "\n",
    "batch['noise_style'] = torch.rand_like(style) / 1e7\n",
    "batch['noise_content'] = torch.rand_like(content) / 1e7\n",
    "output_gt = network(content,style).detach()\n",
    "\n",
    "\n",
    "output_name_gt = '{:s}/{:s}_stylized_{:s}_{:s}_gt{:s}'.format(\n",
    "        output_path, splitext(basename(content_path))[0],\n",
    "        splitext(basename(style_path))[0], word if word is not None else '', save_ext\n",
    "    )\n",
    "save_image(output_gt, output_name_gt)\n",
    "    \n",
    "\n",
    "\n",
    "img_feature_old = clip_model.encode_image(torch.nn.functional.interpolate(\\\n",
    "                                               output_gt,(224,224)))\n",
    "\n",
    "content_gt= batch['content'].clone().detach()\n",
    "style_gt= batch['style'].clone().detach()\n",
    "\n",
    "step=10\n",
    "for i in range(step):\n",
    "    batch['content'] = (batch['content'] + batch['noise_content']).detach() # noised input\n",
    "    batch['content'].requires_grad_(True)\n",
    "    batch['content'].retain_grad()\n",
    "\n",
    "    batch['style'] = (batch['style'] + batch['noise_style']).detach()\n",
    "    batch['style'].requires_grad_(True)\n",
    "    batch['style'].retain_grad()\n",
    "    \n",
    "    output_previous = network(batch['content'] ,batch['style'])\n",
    "    \n",
    "    ########################### ATTACK NOISE #############################\n",
    "    img_feature = clip_model.encode_image(torch.nn.functional.interpolate(\\\n",
    "                                           output_previous,(224,224)))\n",
    "    img_feature_delta = img_feature - img_feature_old\n",
    "    \n",
    "    loss = nn.CosineSimilarity()(img_feature_delta,text_feature_delta)\n",
    "    # loss = torch.var(output_gt) - torch.var(output_previous)\n",
    "    \n",
    "    loss.backward(retain_graph=True)\n",
    "    print('loss: ',loss)\n",
    "\n",
    "    ########################### ATTACK NOISE #############################\n",
    "\n",
    "\n",
    "    optimal_perturbation_style = eps * torch.sign(batch['style'].grad) # inf norm, attack noise\n",
    "    optimal_perturbation_content = eps * torch.sign(batch['content'].grad) # inf norm, attack noise\n",
    "    eps *= # decay rate\n",
    "\n",
    "    batch['style'].grad.zero_()\n",
    "    batch['content'].grad.zero_()\n",
    "\n",
    "    optimal_perturbation_style_old = batch['noise_style']\n",
    "    optimal_perturbation_content_old = batch['noise_content']\n",
    "\n",
    "    optimal_perturbation_style_new = torch.clip(optimal_perturbation_style_old + optimal_perturbation_style,\\\n",
    "                                                -0.01, 0.01) # manually set\n",
    "    optimal_perturbation_content_new = torch.clip(optimal_perturbation_content_old + optimal_perturbation_content,\\\n",
    "                                                -0.15, 0.15) # manually set\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    batch['style'] = batch['style'] - optimal_perturbation_style_old # clean input\n",
    "    batch['content'] = batch['content'] - optimal_perturbation_content_old\n",
    "\n",
    "    batch['noise_style'] = optimal_perturbation_style_new\n",
    "    batch['noise_content'] = optimal_perturbation_content_new\n",
    "\n",
    "    # save output\n",
    "\n",
    "    output_adv= network(batch['content'].clone().detach() + batch['noise_content'],\\\n",
    "                        batch['style'].clone().detach() + batch['noise_style'] )\n",
    "\n",
    "\n",
    "    output_name_adv = '{:s}/{:s}_stylized_{:s}_{:s}_{:s}{:s}'.format(\n",
    "        output_path, splitext(basename(content_path))[0],\n",
    "        splitext(basename(style_path))[0], word if word is not None else '', str(i), save_ext\n",
    "    )\n",
    "    save_image(output_adv, output_name_adv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
