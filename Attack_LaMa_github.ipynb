{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "from saicinpainting.evaluation.utils import move_to_device\n",
    "\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ['VECLIB_MAXIMUM_THREADS'] = '1'\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = '1'\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "import yaml\n",
    "from omegaconf import OmegaConf\n",
    "from torch.utils.data._utils.collate import default_collate\n",
    "\n",
    "from saicinpainting.training.data.datasets import make_default_val_dataset\n",
    "from saicinpainting.training.trainers import load_checkpoint\n",
    "from saicinpainting.utils import register_debug_signal_handlers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "import clip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path='./configs/prediction/'\n",
    "config_name='default.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_config = OmegaConf.load(config_path+config_name)\n",
    "predict_config.model.path = 'LaMa_models/lama-celeba-hq/lama-fourier'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(predict_config.device) # cuda\n",
    "train_config_path = os.path.join(predict_config.model.path, 'config.yaml')\n",
    "with open(train_config_path, 'r') as f:\n",
    "    train_config = OmegaConf.create(yaml.safe_load(f))\n",
    "train_config.training_model.predict_only = True\n",
    "train_config.visualizer.kind = 'noop'\n",
    "out_ext = predict_config.get('out_ext', '.png')\n",
    "checkpoint_path = os.path.join(predict_config.model.path, \n",
    "                               'models', \n",
    "                               predict_config.model.checkpoint)\n",
    "model = load_checkpoint(train_config, checkpoint_path, strict=False, map_location='cpu')\n",
    "model.freeze()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=\"cuda\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out_dir = 'sample_out/'\n",
    "\n",
    "image = cv.imread('/image.jpg')\n",
    "image = cv.resize(image,(256,256))\n",
    "cv.imwrite(out_dir + 'gt.png',image)\n",
    "\n",
    "img = np.transpose(image[:,:,::-1],(2,0,1)) \n",
    "img = np.ascontiguousarray(img).astype(np.float32)\n",
    "img_t = torch.tensor(img,requires_grad = True).unsqueeze(0)# 1,3,H,W\n",
    "mask = torch.zeros_like(img_t[0,0],dtype=torch.float32)\n",
    "shape1 = int(mask.shape[-1] * 0.15)\n",
    "shape2 = int(mask.shape[-1] * 0.90)\n",
    "\n",
    "shape3 = int(mask.shape[-1] * 0.15)\n",
    "shape4 = int(mask.shape[-1] * 0.85)\n",
    "\n",
    "mask[shape1:shape2,shape3:shape4] = 1\n",
    "\n",
    "mask_t = torch.tensor(mask,requires_grad = True).unsqueeze(0).unsqueeze(0) # 1,1,H,W\n",
    "image_masked = image.copy()\n",
    "image_masked[shape1:shape2,shape3:shape4] *= 0\n",
    "cv.imwrite(out_dir + 'masked.png',image_masked)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_adv={}\n",
    "batch_adv['image'] = img_t.cuda()/255 \n",
    "batch_adv['image'] += torch.rand_like(batch_adv['image']) / 1e7\n",
    "batch_adv['original_image'] = img_t.cuda()/255 \n",
    "batch_adv['image'].retain_grad()\n",
    "batch_adv['mask'] = mask_t.cuda()\n",
    "batch_adv['attack_noise'] = torch.zeros_like(batch_adv['image'])\n",
    "\n",
    "\n",
    "eps = \n",
    "word = \n",
    "word_original =\n",
    "step = \n",
    "dacay_rate = \n",
    "\n",
    "word_token = clip.tokenize([word, word_original]).cuda()\n",
    "text_feature = clip_model.encode_text(word_token)\n",
    "text_feature_delta = text_feature[:1] - text_feature[1:2]\n",
    "\n",
    "img_feature_old = clip_model.encode_image(torch.nn.functional.interpolate(\\\n",
    "                                               model(batch_adv.copy())['predicted_image'].detach(),(224,224)))\n",
    "\n",
    "batch_copy = batch_adv.copy()\n",
    "img_inp0 = model(batch_copy)['predicted_image'].detach().cpu().numpy()\n",
    "img_inp0 = np.transpose(img_inp0[0], (1,2,0))\n",
    "img_inp0 = np.clip(img_inp0 * 255, 0, 255).astype('uint8')\n",
    "\n",
    "cv.imwrite('/default.png',img_inp0[:,:,::-1])\n",
    "\n",
    "\n",
    "for i in range(step):\n",
    "    img_feature = clip_model.encode_image(torch.nn.functional.interpolate(\\\n",
    "                                               model(batch_adv)['predicted_image'],(224,224)))\n",
    "        \n",
    "    img_feature_delta = img_feature - img_feature_old\n",
    "    \n",
    "\n",
    "    loss = nn.CosineSimilarity()(img_feature_delta,text_feature_delta)\n",
    "    print('loss: ',loss)\n",
    "\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimal_perturbation = eps * torch.sign(batch_adv['image'].grad) # inf norm, attack noise\n",
    "    eps *= dacay_rate\n",
    "    optimal_perturbation_old = batch_adv['attack_noise']\n",
    "\n",
    "    optimal_perturbation_new = torch.clip(optimal_perturbation_old + optimal_perturbation, -0.01, 0.01)\n",
    "\n",
    "    batch_adv['image']-=optimal_perturbation_old\n",
    "    batch_adv['attack_noise'] = optimal_perturbation_new\n",
    "\n",
    "\n",
    "    batch_adv['image']+=optimal_perturbation_new\n",
    "    batch_adv['image'].grad.zero_()\n",
    "\n",
    "    adv_batch1 = batch_adv.copy()\n",
    "    adv_batch1['image'] = (batch_adv['image'] + optimal_perturbation)\n",
    "\n",
    "    batch_adv1 = model(adv_batch1)\n",
    "    img_inp_adv1 = (batch_adv1['predicted_image'] * batch_adv1['mask'] +\\\n",
    "                    batch_adv['original_image'] * (1 - batch_adv1['mask'])).detach().cpu().numpy()\n",
    "    img_inp_adv1 = np.transpose(img_inp_adv1[0], (1,2,0))\n",
    "    img_inp_adv1 = np.clip(img_inp_adv1 * 255, 0, 255).astype('uint8')\n",
    "    cv.imwrite('sample_out/'+word+str(i)+'.png',img_inp_adv1[:,:,::-1])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
